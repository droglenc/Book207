# Model Comparison {#ModelComparison}

```{r ModelComparison-setup, include=FALSE}
## Set the random number seed for consistent jittering
set.seed(34353)
## Aquaculture data
aqua <- read.csv("https://raw.githubusercontent.com/droglenc/NCData/master/BOD.csv") %>%
  mutate(srcjit=round(jitter(as.numeric(factor(src)),0.25),4))
aqua_mns <- group_by(aqua,src) %>% summarize(mn=mean(BOD))
aqua <- left_join(aqua,aqua_mns,by="src") %>%
  mutate(gmn=mean(BOD),
         residF=BOD-mn,
         residS=BOD-gmn)
```

## Competing Models
### General
Many hypothesis tests can be cast in a framework of competing models. In this module we will cast the familiar 2-sample t-test in this framework which will then serve as a conceptual foundation for all other models in this course.

The two competing models are generically called the *simple* and *full* models (Table \@ref(tab:ModelDifferences)). The simple model is simpler than the full model in the sense that it has fewer parameters. However, the simple model fits the data "worse" than the full model. Thus, determining which model to use becomes a question of balancing "fit" (full model fits better than the simple model) with complexity (full model is more complex than the simple model). Because the simple model corresponds to H<sub>0</sub> and the full model corresponds to H<sub>A</sub>, deciding which model to use is the same as deciding which hypothesis is supported by the data.

&nbsp;

```{r ModelDifferences, echo=FALSE}
data.frame(Model=c("Simple","Full"),
           Parameters=c("Fewer","More"),
           "Residual df"=c("More","Less"),
           "Relative Fit"=c("Worse","Better"),
           "Residual SS"=c("Larger","Smaller"),
           "Hypothesis"=c("Null","Alternative"),
           check.names=FALSE) %>%
knitr::kable(booktabs=TRUE,caption="Differences between the two generic model types.") %>%
  kableExtra::kable_classic("hover",full_width=FALSE,html_font=khfont) %>%
  kableExtra::row_spec(0,bold=TRUE) %>%
  kableExtra::column_spec(1,bold=TRUE)
```

&nbsp;

### 2-Sample t-Test
Recall that H<sub>0</sub> in a two-sample t-test is that the two population means do not differ (i.e., they are equal). In this hypothesis, if the two means do not differ than a single mean would adequately represent both groups. The general model from Section \@ref(what-is-a-model) ^[Generally, Observation = Model Prediction + Error] could be specified for this situation as

$$ Y_{ij} = \mu + \epsilon_{ij} $$

where $Y_{ij}$ is the $j$th observation of the response variable in the $i$th group, $\mu$ is the population grand mean, and $\epsilon_{ij}$ is the "error" for the $j$th observation in the $i$th group. This model means that &mu; is the predicted response value for each observation and the model looks like the red line in Figure \@ref(fig:Models1).

```{r Models1, echo=FALSE, fig.cap='Biological oxygen demand versus sample location with group means shown as blue horizontal segments and the grand mean shown as a red horizontal segment.'}
ggplot(data=aqua) +
  geom_crossbar(mapping=aes(x=src,y=mn,ymin=mn,ymax=mn),
                width=0.25,color=clr_full) +
  geom_crossbar(mapping=aes(x=1.5,y=gmn,ymin=gmn,ymax=gmn),
                width=1.25,color=clr_simple) +
  geom_point(mapping=aes(x=srcjit,y=BOD),alpha=0.5) +
  labs(y="Biological Oxygen Demand",x="Water Sample Location") +
  theme_NCStats()
```

&nbsp;

In contrast, H<sub>A</sub> in the 2-sample t-test is that the two population means differ (i.e., they are not equal). This hypothesis suggests that two separate means are needed to predict observations in the separate groups. The model for this situation is

$$ Y_{ij} = \mu_{i} + \epsilon_{ij} $$

where $\mu_{i}$ is the population mean for the $i$th group. This model means that $\mu_{1}$ is the predicted response value for observations in the first group and $\mu_{2}$ is the predicted response value for observations in the second group. This model looks like the two blue lines in Figure \@ref(fig:Models1).

Thus, for a 2-sample t-test, the **simple model** corresponds to H<sub>0</sub>: $\mu_{1}=\mu_{2}$ (=$\mu$), has fewer parameters (i.e., requires only one mean; the red line in the plots above), and fits "worse."^[We will be more objective in the following sections, but an examination of the plot above clearly shows that the red line does not represent the observations well.] In contrast, the **full model** corresponds to H<sub>A</sub>: $\mu_{1}\ne\mu_{2}$, has more parameters (i.e., requires two means; the blue lines in the plots above), and fits "better."

In the ensuing sections we develop a method to determine if the improvement in "fit" is worth the increase in "complexity."

&nbsp;

## Measuring Increase in Fit
### SS<sub>Total</sub> and SS<sub>Within</sub>
Using the residual sum-of-squares (RSS) to measure the lack-of-fit of a model was introduced in Section \@ref(residual-sum-of-squares). Here we apply that concept to measure the lack-of-fit of the simple and full models, which we will then use to see how much "better" the full model fits than the simple model.

The RSS for the simple model using just the grand mean is called SS<sub>Total</sub> and is computed with

$$ \text{SS}_{\text{Total}} = \sum_{i=1}^{I}\sum_{j=1}^{n_{i}}\left(Y_{ij}-\bar{Y}_{\cdot\cdot}\right)^{2} $$

where $I$ is the number of groups (=2 in a 2-sample t-test), $n_{i}$ is the sample size in the $i$th group, and $\bar{Y}_{\cdot\cdot}$ is the sample grand mean as computed with 

$$ \bar{Y}_{\cdot\cdot}= \frac{\sum_{i=1}^{I}\sum_{j=1}^{n_{i}}Y_{ij}}{n} $$

where $n$ is the sample size across all groups. The $\bar{Y}_{\cdot\cdot}$ is used here because it is an estimate of the population grand mean, $\mu$, which is used to make predictions in this simple model.

The formula for SS<sub>Total</sub> may look daunting but it is just the sum of the squared residuals computed from each observation relative to the grand mean (Figure \@ref(fig:FullModelResids1)).

```{r FullModelResids1, echo=FALSE, fig.cap='Biological oxygen demand versus sample location with the grand mean shown as a red horizontal segment. Residuals from the grand mean are shown by red vertical dashed lines. The sum of these residuals is SS<sub>Total</sub>.'}
ggplot(data=aqua) +
  geom_crossbar(mapping=aes(x=src,y=mn,ymin=mn,ymax=mn),color="transparent") +
  geom_crossbar(mapping=aes(x=1.5,y=gmn,ymin=gmn,ymax=gmn),
                width=1.25,color=clr_simple) +
  geom_point(mapping=aes(x=srcjit,y=BOD),alpha=0.5) +
  geom_segment(data=aqua,mapping=aes(x=srcjit,xend=srcjit,y=BOD,yend=gmn),
               color=clr_simple,linetype="dashed",alpha=0.5) +
  labs(y="Biological Oxygen Demand",x="Water Sample Location",
       title=bquote("Residuals for"~SS[Total])) +
  annotate(geom="text",x=1.7,y=6.5,
           label=expression(plain("One of")~Y[ij]),parse=TRUE,
           size=4.5,hjust=-0.1,color=lbl_clr) +
  annotate(geom="segment",x=1.7,y=6.5,xend=aqua$srcjit[7],yend=aqua$BOD[7],
           arrow=ARROW,color=lbl_clr) +
  annotate(geom="text",x=1,y=9,
           label=expression(bar(Y)~plain(..)),parse=TRUE,
           size=4.5,hjust=1.1,vjust=-0.1,color=lbl_clr) +
  annotate(geom="segment",x=1,y=9,xend=1.5,yend=aqua$gmn[1],
           arrow=ARROW,color=lbl_clr) +
  theme_NCStats() +
  theme(aspect.ratio=1)
```

&nbsp;

The RSS for the full model using separate group means is called SS<sub>Within</sub> and is computed with

$$ \text{SS}_{\text{Within}} = \sum_{i=1}^{I}\sum_{j=1}^{n_{i}}\left(Y_{ij}-\bar{Y}_{i\cdot}\right)^{2} $$

where $\bar{Y}_{i\cdot}$ are the sample group means as computed with 

$$ \bar{Y}_{\cdot\cdot} = \frac{\sum_{j=1}^{n_{i}}Y_{ij}}{n_{i}} $$

The $\bar{Y}_{i\cdot}$ are used here because they are an estimate of the population group means, $\mu_{i}$, which are used to make predictions in this full model. Again, the formula for SS<sub>Within</sub> may look imposing but it is just the sum of the squared residuals computed from each observation to the observation's group mean (Figure \@ref(fig:SimpleModelResids1)).

```{r SimpleModelResids1, echo=FALSE, fig.cap='Biological oxygen demand versus sample location with the group means shown as blue horizontal segments. Residuals from the group means are shown by blue vertical dashed lines. The sum of these residuals is SS<sub>Within</sub>.'}
ggplot(data=aqua) +
  geom_crossbar(mapping=aes(x=src,y=mn,ymin=mn,ymax=mn),
                width=0.25,color=clr_full) +
  geom_point(mapping=aes(x=srcjit,y=BOD),alpha=0.5) +
  geom_segment(data=aqua,mapping=aes(x=srcjit,xend=srcjit,y=BOD,yend=mn),
               color=clr_full,linetype="dashed",alpha=0.5) +
  labs(y="Biological Oxygen Demand",x="Water Sample Location",
       title=bquote("Residuals for"~SS[Within])) +
  annotate(geom="text",x=1.7,y=6.5,
           label=expression(plain("One of")~Y[ij]),parse=TRUE,
           size=4.5,hjust=-0.1,color=lbl_clr) +
  annotate(geom="segment",x=1.7,y=6.5,xend=aqua$srcjit[7],yend=aqua$BOD[7],
           arrow=ARROW,color=lbl_clr) +
  annotate(geom="text",x=1.2,y=8,
           label=expression(bar(Y)[i]~plain(.)),parse=TRUE,
           size=4.5,hjust=1.1,vjust=-0.1,color=lbl_clr) +
  annotate(geom="segment",x=1.2,y=8,xend=1.1,yend=aqua$mn[1],
           arrow=ARROW,color=lbl_clr) +
  annotate(geom="segment",x=1.2,y=8,xend=1.9,yend=aqua$mn[nrow(aqua)],
           arrow=ARROW,color=lbl_clr) +
  theme_NCStats() +
  theme(aspect.ratio=1)
```

&nbsp;

Thus, SS<sub>Total</sub> measures the lack-of-fit of the grand mean to the data or the lack-of-fit of the simple model. SS<sub>Within</sub>, in contrast, measures the lack-of-fit of the group means to the data or the lack-of-fit of the full model.

In this example, SS<sub>Total</sub>=`r formatC(sum(aqua$residS^2),format="f",digits=2)` and SS<sub>Within</sub>=`r formatC(sum(aqua$residF^2),format="f",digits=2)`. Because SS<sub>Within</sub> is less than SS<sub>Total</sub> that means that the full model that uses $\mu_{i}$ fits the data better than the simple model that uses just $\mu$.

However, we knew that this was going to happen as the full model always fits better. What we need now is a measure of how much better the full model fits or, equivalently, a measure of how much the lack-of-fit was reduced by using the full model rather than the simple model.


::: {.tip data-latex=""}
SS<sub>Total</sub> measures the lack-of-fit of the simple model, whereas SS<sub>Within</sub> measures the lack-of-fit of the full model.
:::

::: {.tip data-latex=""}
The full model always fits better than the simple model, even if by just a small amount.
:::

### SS<sub>Among</sub>
A useful property of SS<sub>Total</sub> is that it "partitions" into two parts according to the following simple formula

$$ \text{SS}_{\text{Total}} = \text{SS}_{\text{Within}} + \text{SS}_{\text{Among}} $$

A quick re-arrangement of the partitioning of SS<sub>Total</sub> shows that

$$ \text{SS}_{\text{Among}} = \text{SS}_{\text{Total}} - \text{SS}_{\text{Within}} $$

Thus, SS<sub>Among</sub> records how much the lack-of-fit was reduced by using the full model rather than the simple model. In other words, SS<sub>Among</sub> records how much "better" the full model fits the data than the simple model. In our example, SS<sub>Among</sub>=`r formatC(sum(aqua$residS^2),format="f",digits=2)`-`r formatC(sum(aqua$residF^2),format="f",digits=2)`=`r formatC(sum(aqua$residS^2)-sum(aqua$residF^2),format="f",digits=2)`. Thus, the residual SS from the simple model was reduced by `r formatC(sum(aqua$residS^2)-sum(aqua$residF^2),format="f",digits=2)` when the full model was used.

SS<sub>Among</sub> can also be thought of in a different way. It can be algebraically shown that 

$$ \text{SS}_{\text{Among}} = \sum_{i=1}^{I}n_{i}\left(\bar{Y}_{i\cdot}-\bar{Y}_{\cdot\cdot}\right)^{2} $$

Again, this looks complicated, but the main part to focus on is $\bar{Y}_{i\cdot}-\bar{Y}_{\cdot\cdot}$, which shows that SS<sub>Among</sub> is primarily concerned with measuring the distance between the group means (i.e., $\bar{Y}_{i\cdot}$) and the grand mean (i.e., $\bar{Y}_{\cdot\cdot}$; Figure \@ref(fig:BothModelResids)).

```{r BothModelResids, echo=FALSE, fig.cap='Mean biological oxygen demand versus sample location with the grand mean shown as a red horizontal segment and the group means shown as blue horizontal segments. Residuals between the group means and the grand mean are shown by black vertical dashed lines. The sum of these residuals scaled by the group sample sizes is SS<sub>Among</sub>.'}
ggplot(data=aqua) +
  geom_crossbar(mapping=aes(x=src,y=mn,ymin=mn,ymax=mn),
                width=0.25,color=clr_full) +
  geom_crossbar(mapping=aes(x=1.5,y=gmn,ymin=gmn,ymax=gmn),
                width=1.25,color=clr_simple) +
  geom_point(mapping=aes(x=srcjit,y=BOD),alpha=0.01,color="white") +
  geom_segment(data=aqua,mapping=aes(x=src,xend=src,y=mn,yend=gmn),
               linetype="dashed") +
  labs(y="Biological Oxygen Demand",x="Water Sample Location",
       title=bquote('"Residuals" for'~SS[Among])) +
  annotate(geom="text",x=0.65,y=8,
           label=expression(bar(Y)[i]~plain(.)),parse=TRUE,
           size=4.5,hjust=1.1,vjust=-0.1,color=lbl_clr) +
  annotate(geom="segment",x=0.65,y=8,xend=0.9,yend=aqua$mn[1],
           arrow=ARROW,color=lbl_clr) +
  annotate(geom="segment",x=0.65,y=8,xend=1.9,yend=aqua$mn[nrow(aqua)],
           arrow=ARROW,color=lbl_clr) +
  annotate(geom="text",x=2,y=6.6,
           label=expression(bar(Y)~plain(..)),parse=TRUE,
           size=4.5,hjust=-0.1,vjust=1.1,color=lbl_clr) +
  annotate(geom="segment",x=2,y=6.5,xend=1.5,yend=aqua$gmn[1],
           arrow=ARROW,color=lbl_clr) +
  theme_NCStats() +
  theme(aspect.ratio=1)
```

&nbsp;

From the figure above, it is seen that SS<sub>Among</sub> will increase as the group means become more different. This can also be seen by increasing or decreasing the difference between the group means and the grand mean in the interactive graphic below. [*Note how SS<sub>Among</sub> (and SS<sub>Total</sub>) change.*]

```{r echo=FALSE, out.width="75%"}
knitr::include_app("https://derek-ogle.shinyapps.io/App_SSTotal_Partitioning/",height='800px')
```

&nbsp;

So, SS<sub>Among</sub> is immensely useful -- it is a measure of "benefit" that will be used in a "benefit-to-cost" ratio that will ultimately be the "signal" that will be used in a "signal-to-noise" ratio. These ratios are introduced in Sections \@ref(noise-variances) and \@ref(signal-variance-benefit-to-cost). However, next we discuss how to measure the "cost" of using the more complex full model.

::: {.tip data-latex=""}
SS<sub>Among</sub> is the **benefit** (i.e., reduction in lack-of-fit) from using the full model and will ultimately be used to measure the **signal** (i.e., relative difference in group means) in the data.
:::


## Measuring Increase in Complexity
In this example, df<sub>Total</sub>=`r nrow(aqua)`-1 because there is one parameter (the grand mean) in the simple model, and df<sub>Within</sub>=`r nrow(aqua)`-2 because there are two parameters (the group means) in the full model. The full model uses more parameters and, thus, the residual degrees-of-freedom is reduced -- there is a "cost" to using the full model over the simple model. We need a measure of this "cost".^[The "cost" is obviously 1 in this simple case.]

Interestingly df<sub>Total</sub> partitions in the same way as SS<sub>Total</sub>; i.e., 

$$ \text{df}_{\text{Total}} = \text{df}_{\text{Within}} + \text{df}_{\text{Among}} $$

A quick re-arrangement of the partitioning of df<sub>Total</sub> shows that

$$ \text{df}_{\text{Among}} = \text{df}_{\text{Total}} - \text{df}_{\text{Within}} $$

In this case, df<sub>Among</sub>=`r nrow(aqua)-1`-`r nrow(aqua)-2`=1.

Thus, df<sub>Among</sub> is the degrees-of-freedom that were "lost" or "used" when the more complex full model was used compared to the less complex simple model. The df<sub>Among</sub> is also the **difference in number of parameters**  between the full and simple models. In other words, df<sub>Among</sub> is how much more complex (in terms of number of parameters) the full model is compared to the simple model. Thus, df<sub>Among</sub> measures the **cost** of using the full model rather than the simple model.

::: {.tip data-latex=""}
df<sub>Among</sub> is the extra **cost** (i.e., loss of df due to more parameters to estimate) from using the full rather than simple model.
:::

## "Noise" Variances
MS<sub>Total</sub> and MS<sub>Within</sub> are measures of the variance ^[As discussed in Section \@ref(mean-squares), SS are not true variances until they are divided by their df and become mean-squares (MS).] of **individuals** around the grand mean and group means, respectively. Thus, MS<sub>Total</sub> measures the variance or "noise" around the simple model, whereas MS<sub>Within</sub> measures the variance or "noise" around the full model.

::: {.tip data-latex=""}
MS<sub>Total</sub> and MS<sub>Within</sub> measure "noise" -- i.e., variability of observations around a model.
:::

Note that

$$ \text{MS}_{\text{Total}} = \frac{\text{SS}_{\text{Total}}}{\text{df}_{\text{Total}}} = \frac{\sum_{i=1}^{I}\sum_{j=1}^{n_{i}}\left(Y_{ij}-\bar{Y}_{\cdot\cdot}\right)^{2}}{n-1} $$

The double summation simply means to "sum across all individuals." With this it should be seen that this is same as the variance ($s^{2}$) from your introductory statistics course. In other words MS<sub>Total</sub> is just the variability of the individuals around a mean that ignores that there are groups.

::: {.tip data-latex=""}
MS<sub>Total</sub>=$s^{2}$
:::

Similarly,

$$ \text{MS}_{\text{Within}} = \frac{\text{SS}_{\text{Within}}}{\text{df}_{\text{Within}}} = \frac{\sum_{i=1}^{I}\sum_{j=1}^{n_{i}}\left(Y_{ij}-\bar{Y}_{i\cdot}\right)^{2}}{\sum_{i=1}^{I}n_{i}-I} $$

It is not hard to show algebraically (and for just two groups) that the numerator (top part) is $n_{1}s_{1}^{2}+n_{2}s_{2}^{2}$ and the denominator (bottom part) is $n_{1}+n_{2}-2$. With this numerator and denominator, it is evident that MS<sub>Within</sub> (for two groups) is the same as the pooled sample variance ($s_{p}^{2}$) from the 2-sample t-test.

::: {.tip data-latex=""}
MS<sub>Within</sub>=$s_{p}^{2}$ from the 2-sample t-test.
:::

## "Signal" Variance (Benefit-to-Cost)
Of course SS<sub>Among</sub> divided by df<sub>Among</sub> will be MS<sub>Among</sub>. However, while MS<sub>Among</sub> is still a variance, it has a very different interpretation.

MS<sub>Among</sub> is NOT a variance of *individuals*, rather it is a variance of *sample means*. Sample means can vary (i.e., not be equal) for two reasons -- random sampling variability^[That is sample means will vary from sample to sample even if the population means are not different.] or the population means are really different.^[If the population means are different then one would expect the means from samples of those populations to also be different.] In other words, MS<sub>Among</sub> -- the variance among means -- is a combination of "noise" and "signal." Our goal (next) is to disentangle these two reasons for why the sample means differ to determine if there is a real "signal" or not.

Additionally, MS<sub>Among</sub> is a ratio of the "benefit" (i.e., SS<sub>Among</sub>) to the "cost" (i.e., df<sub>Among</sub>) of using the full model over the simple model. So MS<sub>Among</sub> scales the benefit to the cost of using the full model.

&nbsp;

## Ratio of Variances (Signal-to-Noise)
From the above discussion we have a measure of potential "signal" in MS<sub>Among</sub>, but it is contaminated with some "noise." Additionally, we have a measure of just "noise" around the full model (the model representing the "signal") in MS<sub>Within</sub>. A ratio of this "signal" to "noise" is called an F test statistic; i.e.,

&nbsp;

$$ \text{F}=\frac{\text{MS}_{\text{Among}}}{\text{MS}_{\text{Within}}} = \frac{\text{Signal}}{\text{Noise}} = \frac{\text{Variance Explained by Full Model}}{\text{Variance Unexplained by Full Model}} $$

&nbsp;

If the F-ratio is "large," then "signal" is stronger than the "noise." A large F-ratio also means that more variability was explained than was unexplained by the full model. Thus, large F-ratio values mean that the full model fits the data significantly better than the simple model, even considering the increased complexity of the full model.

The question now becomes "when is the F-ratio considered large enough to reject the simple model and conclude that the full model is significantly better?" This question is answered by comparing the F-ratio test statistic to an F-distribution.

An F-distribution ^[An F-distribution occurs whenever the ratio of two variances is calculated.] is right-skewed, with the exact shape of the distribution dictated by two separate df -- called the numerator and denominator df, respectively. The numerator df is equal to the df used in MS<sub>Among</sub>, whereas the denominator df is equal to the df used in MS<sub>Within</sub>. The p-value is always computed as the area under the F-distribution curve to the *right* of the observed F-ratio test statistic (Figure \@ref(fig:Fpvalue)).^[If the F-ratio is computed by hand, then `distrib()` with `distrib="f"`, `df1=`, `df2=`, and `lower.tail=FALSE` may be used to calculate the corresponding p-value.]

```{r Fpvalue, echo=FALSE, fig.cap='An F distribution with 4 numerator and 20 denominator degrees-of-freedom. The shaded area demonstrates calculating a p-value for an F-ratio of 2.2.'}
dfA <- 4
dfW <- 20

b <- ggplot(data.frame(x=c(-Inf,Inf)),mapping=aes(x=x)) +
  stat_function(fun=df,args=list(df1=dfA,df2=dfW),xlim=c(0,5),
                geom="line",color="black",size=1.1) +
  scale_y_continuous(expand=expansion(mult=c(0,0.04))) +
  scale_x_continuous(expand=expansion(mult=c(0,0))) +
  theme_NCStats() +
  theme(axis.title.y=element_blank(),axis.title.x=element_text(size=16),
        axis.text.x=element_text(size=12),axis.text.y=element_blank())

b + labs(x="F-Ratio") +
  stat_function(fun=df,args=list(df1=dfA,df2=dfW),xlim=c(2.2,5),
                geom="area",fill="gray20",color="black",size=1.1) +
  annotate(geom="text",x=2.8,y=df(2.2,df1=dfA,df2=dfW),
           label="p-value",size=5.5,hjust=-0.1,color=lbl_clr) +
  annotate(geom="segment",x=2.2,y=df(2.2,df1=dfA,df2=dfW),
           xend=2.8,yend=df(2.2,df1=dfA,df2=dfW),
           arrow=ARROW,color=lbl_clr)
```

&nbsp;

From all of the previous discussion, it can be seen that ...

* a small p-value comes from a large F-ratio, which comes from
* a large MS<sub>Among</sub> relative to MS<sub>Within</sub>, which means that 
* the full model explains more variability than is left unexplained and that the "signal" is much greater than the "noise", which means that
* the full model does fit significantly better than the simple model (even given the increased complexity), and, thus,
* the means are indeed different.

This cascade of measures can be explored with the dynamic graphic below.

```{r echo=FALSE, out.width="80%"}
knitr::include_app("https://derek-ogle.shinyapps.io/App_F_Meaning/",height='800px')
```

::: {.tip data-latex=""}
A **small p-value** results in concluding that the **group means are significantly different**.
:::


## ANOVA Table
The degrees-of-freedom (df), sum-of-squares (SS), mean-squares (MS), F-ratio test statistic (F), and corresponding p-value are summarized in an **analysis of variance (ANOVA) table**.^[An ANOVA table does not necessarily mean that an "analysis of variance" method was used. It turns out that all general linear models are summarized with an ANOVA table, regardless of whether a one- or two-way ANOVA method was used.] The ANOVA table contains rows that correspond to the different measures discussed above: among,^[Labeled as the factor variable in most statistical software packages including R -- that variable was called `src` in this example.] within,^[Labeled as residuals in R and error in other statistical software packages.] and total. The df and SS are shown for each source, but the MS is shown only for the within and among sources because MS<sub>Among</sub>+MS<sub>Within</sub> **&ne;** MS<sub>Total</sub>.

An ANOVA table for the BOD measurements at the inlet and outlet sources to the aquaculture facility is in Table \@ref(tab:ANOVA1). Note that R does not show the total row that most softwares do.

```{r ANOVA1, echo=FALSE}
lm1 <- lm(BOD~src,data=aqua)
aov <- anova(lm1)
knitr::kable(aov,booktabs=TRUE,digits=4,
             caption='An ANOVA table for biological oxygen demand measurements at two locations of the aquaculture facility. Note that the "Total" row is not shown.') %>%
  kableExtra::kable_classic("hover",full_width=FALSE,html_font=khfont) %>%
  kableExtra::row_spec(0,bold=TRUE) %>%
  kableExtra::column_spec(1,bold=TRUE)
```

&nbsp;

These results indicate that H<sub>0</sub> should be rejected (i.e., F-test p-value `r kPvalue(aov[1,"Pr(>F)"],include.p=FALSE,latex=FALSE)`). Thus, the full model fits the data significantly better than the simple model even given the increased complexity and sampling variability. Therefore, there is a significant difference in mean BOD between the two source locations.

In addition to the primary objective of comparing the full and simple models and making a conclusion about the equality of group means, several other items of interest can be identified from an ANOVA table. Using the table above as an example, note that

* the variance within groups is `r formatC(aov$"Mean Sq"[2],format="f",digits=4)` (=MS<sub>Within</sub>=MS<sub>Residuals=$s_{p}^{2}$), and
* the common variance about the mean is `r formatC(sum(aov$"Sum Sq")/sum(aov$Df),format="f",digits=4)` (=MS<sub>Total</sub>=$s^{2}$=$\frac{`r formatC(aov$"Sum Sq"[1],format="f",digits=4)`+`r formatC(aov$"Sum Sq"[2],format="f",digits=4)`}{`r formatC(aov$Df[1],format="f",digits=0)`+`r formatC(aov$Df[2],format="f",digits=0)`}$).

::: {.tip data-latex=""}
SS and df partition, but MS do not! Do not add MS<sub>Among</sub> and MS<sub>Within</sub> to get MS<sub>Total</sub>, instead divide SS<sub>Total</sub> by df<sub>Total</sub>.
:::

&nbsp;

## Two-Sample t-Test Revisited: Using Linear Models
The models for a two-sample t-test can be fit and assessed with `lm()`{.inline}. This function requires the same type of formula for its first argument -- `response~groups`{.inline} -- and a data.frame in the `data=`{.inline} argument as described for `t.test()`{.inline} in Section \@ref(T2analysis). The results of `lm()` should be assigned to an object so that specific results can be selectively extracted from it. For example, the ANOVA table results are extracted with `anova()`{.inline}. In addition, coefficient results^[The coefficient results will be discussed in more detail in Module \@ref(ANOVA1Foundations).] can be extracted with `coef()`{.inline} and `confint()`{.inline}, noting that I like to "column-bind" the coefficients and confidence intervals together for a more succinct representation.

```{r echo=TRUE}
aqua.lm <- lm(BOD~src,data=aqua)
anova(aqua.lm)
cbind(ests=coef(aqua.lm),confint(aqua.lm))
```

From these results, note:

* The p-value in the ANOVA table is the same as that computed from `t.test()`{.inline}.
* The coefficient for `srcoutlet` is the same as the difference in the group means computed with `t.test()`{.inline}.
* The F test statistics in the ANOVA table equals the square of the t test statistic from `t.test()`{.inline}. This is because an F with 1 numerator and v denominator df exactly equals the square of a t with v df.

Thus, the exact same results for a two-sample t-test are obtained whether the analysis is completed in the "traditional" manner (i.e., with `t.test()`{.inline}) or with competing models (i.e., using `lm()`{.inline}). This concept will be extended in subsequent modules.

&nbsp;

## One More Look at MS and F-test
Recall from your introductory statistics course that a sampling distribution is the distribution of a statistic from all possible samples. For example, the Central Limit Theorem states that the distribution of sample means is approximately normal, centered on &mu;, with a standard error of $\frac{\sigma}{\sqrt{n}}$ as long as assumptions about the sample size are met. Further recall that the sampling distribution of the sample means is centered on &mu; because the sample mean is an unbiased estimator of &mu;. Similarly, it is also known that the center of the sampling distribution of $s^{2}$ is equal to $\sigma^{2}$ because $s^{2}$ is an unbiased estimate of $\sigma^{2}$.

MS<sub>Within</sub> and MS<sub>Among</sub> are statistics just as $\bar{x}$ and $s^{2}$ are statistics. Thus, MS<sub>Within</sub> and MS<sub>Among</sub> are subject to sampling variability and have sampling distributions. It can be shown^[This derivation is beyond the scope of this course.] that the center of the sampling distribution of MS<sub>Within</sub> is $\sigma_{p}^{2}$ and the center of the sampling distribution of MS<sub>Among</sub> is

$$ \sigma_{p}^{2} + \frac{\sum_{i=1}^{I}n_{i}\left(\mu_{i}-\mu\right)^{2}}{I-1} $$

Thus, MS<sub>Among</sub> consists of two "sources" of variability. The first source ($\sigma_{p}^{2}$) is the natural variability that exists among individuals (around the group means). The second source $\left(\frac{\sum_{i=1}^{I}n_{i}\left(\mu_{i}-\mu\right)^{2}}{I-1}\right)$ is related to differences among the group means. Therefore, if the group means are all equal -- i.e., $\mu_{1}$=$\mu_{2}$= $\cdots$ = $\mu_{I}$ = $\mu$ -- then the second source of variability is equal to zero and MS<sub>Among</sub> will equal MS<sub>Within</sub>. As soon as the groups begin to differ, the second source of variability will be greater than 0 and MS<sub>Among</sub> will be greater than MS<sub>Within</sub>.

From this, it follows that if the null hypothesis of equal population means is true (i.e., one mean fits all groups), then the center of the sampling distribution of both MS<sub>Within</sub> and MS<sub>Among</sub> is $\sigma_{p}^{2}$. Therefore, if the null hypothesis is true, then the F test-statistic is expected to equal 1, on average, which will always result in a large p-value and a DNR H<sub>0</sub> conclusion. However, if the null hypothesis is false (i.e., separate means are needed for all groups), then the center of the sampling distribution of MS<sub>Within</sub> is $\sigma_{p}^{2}$ but the center of the sampling distribution of MS<sub>Among</sub> is $\sigma_{p}^{2}$ + "something", where the "something" is greater than 0 and gets larger as the means become "more different."  Thus, if the null hypothesis is false then the F test-statistic is expected to be greater than 1 and will get larger as the null hypothesis gets "more false."  This analysis of sampling distribution theory illustrates once again that (1) MS<sub>Among</sub> consists of multiple sources of variability and (2) "large" values of the F test-statistic indicate that the null hypothesis is incorrect.

&nbsp;
