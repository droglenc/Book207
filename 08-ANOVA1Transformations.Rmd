# One-Way Transformations {#ANOVA1Transformations}
A One-Way ANOVA depends on four assumptions being met (see Module \@ref(ANOVA1Assumptions)). If those assumptions are not met, then the results of the one-way ANOVA are invalid. Fortunately, violations of the equality of variances and normality assumptions can often be addressed by transforming the quantitative response variable to a scale where the assumptions are met. For example it is common to use the natural log of the response variable rather than the original response variable.

Besides the obvious reason related to assumption violations, Fox (1997) gave four arguments for why data that is skewed or has unequal variances should be transformed:

* Highly skewed distributions are difficult to examine because most of the observations are confined to a small part of the range of the data.
* Apparently outlying individuals in the direction of the skew are brought in towards the main body of the data when the distribution is made more symmetric. In contrast, unusual values in the direction opposite to the skew can be hidden prior to transforming the data.
* Linear models summarize distributions based on means. The mean of a skewed distribution is not, however, a good summary of its center.
* When a variable has very different degrees of variation in different groups, it becomes difficult to examine the data and to compare differences in levels across the groups.

Identification of an appropriate transformation and understanding the resultant output is the focus of this module.

::: {.tip data-latex=""}
If the assumptions of a one-way ANOVA are not met, then the data may be transformed to a scale where the assumptions are met.
:::

## Power Transformations
The response variable may be transformed by raising it to a particular power, $\lambda$, i.e., $Y^{\lambda}$ (Table \@ref(tab:CommonTranformations)).

&nbsp;

```{r CommonTranformations, echo=FALSE}
tibble(Power=c("&lambda;=0.5","&lambda;=0.33","&lambda;=0.25",
               "&lambda;=0","&lambda;=-1"),
       Name=c("Square Root","Cube Root","Fourth Root","Natural Log","Inverse"),
       Formula=c("$Y^{0.5}=\\sqrt{Y}$","$Y^{0.33}=\\sqrt[3]{Y}$",
                 "$Y^{0.25}=\\sqrt[4]{Y}$","$log(Y)$","$Y^{-1}=\\frac{1}{Y}$"),
       "R Code"=c("<pre style='padding: 2px; display: inline; background: transparent; border: 0;'>df$newvar &lt;- sqrt(df$oldvar)</pre>",
                  "<pre style='padding: 2px; display: inline; background: transparent; border: 0;'>df$newvar &lt;- df$oldvar^(1/3)</pre>",
                  "<pre style='padding: 2px; display: inline; background: transparent; border: 0;'>df$newvar &lt;- df$oldvar^(1/4)</pre>",
                  "<pre style='padding: 2px; display: inline; background: transparent; border: 0;'>df$newvar &lt;- log(df$oldvar)</pre>",
                  "<pre style='padding: 2px; display: inline; background: transparent; border: 0;'>df$newvar &lt;- 1/df$oldvar</pre>")) %>%
  knitr::kable(booktabs=TRUE,escape=FALSE,
               caption="Common power transformations in ANOVAs.") %>%
  kableExtra::kable_classic("hover",full_width=FALSE,html_font=khfont) %>%
  kableExtra::row_spec(c(0,4),bold=TRUE)
```

&nbsp;

Each transformation in Table \@ref(tab:CommonTranformations) "spreads out" small values and "draws in" large values in a distribution. For example, in Figure \@ref(fig:OWATransformLog) the log function is shown as the black curved line, the original histogram of strongly right-skewed data is shown upside-down below the x-axis, and the histogram following the log-transformation is shown sideways left of the y-axis. Two "small" values are shown in red on the x-axis. The log-transformation of these two points is shown by following the two vertical lines from these points to the black log function and then moving horizontally to the y-axis. From this it is seen that these two values that were relatively close together are more spread out after the transformation. Conversely two "large" values are shown in blue on the x-axis. Following the same process it is seen that these two points are closer together following the log transformation.

```{r OWATransformLog, echo=FALSE, fig.width=5, fig.height=5, fig.cap="Demonstration of the result (upper-left) from applying the natural log transformation function (black curve in upper-right) to strongly right-skewed original values (lower-right)."}
x1 <- rlnorm(500,0,1); y1 <- log(x1)  # lognormal data
layout(matrix(c(1,3,0,2),nrow=2,byrow=TRUE))
par(mar=c(0,0.1,0.1,0), mgp=c(0,0,0),las=1,tcl=-0.2)
y.hist <- hist(y1,breaks=20,plot=FALSE)
barplot(-y.hist$counts,horiz=TRUE,space=c(0,0),col="gray90",
        xaxt="n",yaxt="n",xlab="",ylab="",xaxs="i",yaxs="i")
mtext("log(Y)",side=3,line=-2)
par(mar=c(0.1,0,0,0.1), mgp=c(0,0,0),las=1,tcl=-0.2)
x.hist <- hist(x1,breaks=20,plot=FALSE)
barplot(-x.hist$counts,space=c(0,0),col="gray90",
         xaxt="n",yaxt="n",xlab="",ylab="",xaxs="i",yaxs="i")
mtext("Y",side=3,line=-4)
par(mar=c(0,0,0,0), mgp=c(0,0,0),las=1,tcl=-0.2)
curve(log(x),min(x1),max(x1),n=501,lwd=2,col="black",
      xlab="",ylab="",bty="n",xaxt="n",yaxt="n",tck=0,xaxs="i",yaxs="i")
xs <- c(0.5,1,5,10)
ys <- log(xs)
segments(xs[1],-10,xs[1],ys[1],col=clrs2[2],lty="dashed")
segments(xs[1],ys[1],0,ys[1],col=clrs2[2],lty="dashed")
points(xs[1],min(y1),col=clrs2[2],pch=16,xpd=NA)
points(min(x1),ys[1],col=clrs2[2],pch=16,xpd=NA)
segments(xs[2],-10,xs[2],ys[2],col=clrs2[2],lty="dashed")
segments(xs[2],ys[2],0,ys[2],col=clrs2[2],lty="dashed")
points(xs[2],min(y1),col=clrs2[2],pch=16,xpd=NA)
points(min(x1),ys[2],col=clrs2[2],pch=16,xpd=NA)
segments(xs[3],-10,xs[3],ys[3],col=clrs2[1],lty="dashed")
segments(xs[3],ys[3],0,ys[3],col=clrs2[1],lty="dashed")
points(xs[3],min(y1),col=clrs2[1],pch=16,xpd=NA)
points(min(x1),ys[3],col=clrs2[1],pch=16,xpd=NA)
segments(xs[4],-10,xs[4],ys[4],col=clrs2[1],lty="dashed")
segments(xs[4],ys[4],0,ys[4],col=clrs2[1],lty="dashed")
points(xs[4],min(y1),col=clrs2[1],pch=16,xpd=NA)
points(min(x1),ys[4],col=clrs2[1],pch=16,xpd=NA)
```

&nbsp;

As seen in Figure \@ref(fig:OWATransformLog), applying the log transformation to all values in the original strongly right-skewed distribution resulted in a distribution that was approximately normal. All transformations may not achieve normality. For example, the same process shown in Figure \@ref(fig:OWATransformLog) is repeated in Figure \@ref(fig:OWATransformSqrt) for a square-root transformation. A comparison of the two figures shows that the square-root transformation function is less curved, it spreads out relatively small values less and draws in relative larger values less, and it does not transform the original strongly right-skewed distribution to an approximately normal distribution. Thus, the square-root transformation is not a "strong enough" transformation to normalize this strongly right-skewed original distribution.

```{r OWATransformSqrt, echo=FALSE, fig.width=5, fig.height=5, fig.cap="Demonstration of the result (upper-left) from applying the square root transformation function (black curve in upper-right) to strongly right-skewed original values (lower-right). The original values here are the same as those in the previous Figure."}
y1 <- sqrt(x1)
layout(matrix(c(1,3,0,2),nrow=2,byrow=TRUE))
par(mar=c(0,0.1,0.1,0), mgp=c(0,0,0),las=1,tcl=-0.2)
y.hist <- hist(y1,breaks=20,plot=FALSE)
barplot(-y.hist$counts,horiz=TRUE,space=c(0,0),col="gray90",
        xaxt="n",yaxt="n",xlab="",ylab="",xaxs="i",yaxs="i")
mtext("sqrt(Y)",side=3,line=-2)
par(mar=c(0.1,0,0,0.1), mgp=c(0,0,0),las=1,tcl=-0.2)
x.hist <- hist(x1,breaks=20,plot=FALSE)
barplot(-x.hist$counts,space=c(0,0),col="gray90",
         xaxt="n",yaxt="n",xlab="",ylab="",xaxs="i",yaxs="i")
mtext("Y",side=3,line=-4)
par(mar=c(0,0,0,0), mgp=c(0,0,0),las=1,tcl=-0.2)
curve(sqrt(x),min(x1),max(x1),n=501,lwd=2,col="black",
      xlab="",ylab="",bty="n",xaxt="n",yaxt="n",tck=0,xaxs="i",yaxs="i")
xs <- c(0.5,1,5,10)
ys <- sqrt(xs)
segments(xs[1],-10,xs[1],ys[1],col=clrs2[2],lty="dashed")
segments(xs[1],ys[1],0,ys[1],col=clrs2[2],lty="dashed")
points(xs[1],min(y1),col=clrs2[2],pch=16,xpd=NA)
points(min(x1),ys[1],col=clrs2[2],pch=16,xpd=NA)
segments(xs[2],-10,xs[2],ys[2],col=clrs2[2],lty="dashed")
segments(xs[2],ys[2],0,ys[2],col=clrs2[2],lty="dashed")
points(xs[2],min(y1),col=clrs2[2],pch=16,xpd=NA)
points(min(x1),ys[2],col=clrs2[2],pch=16,xpd=NA)
segments(xs[3],-10,xs[3],ys[3],col=clrs2[1],lty="dashed")
segments(xs[3],ys[3],0,ys[3],col=clrs2[1],lty="dashed")
points(xs[3],min(y1),col=clrs2[1],pch=16,xpd=NA)
points(min(x1),ys[3],col=clrs2[1],pch=16,xpd=NA)
segments(xs[4],-10,xs[4],ys[4],col=clrs2[1],lty="dashed")
segments(xs[4],ys[4],0,ys[4],col=clrs2[1],lty="dashed")
points(xs[4],min(y1),col=clrs2[1],pch=16,xpd=NA)
points(min(x1),ys[4],col=clrs2[1],pch=16,xpd=NA)
```

&nbsp;

The square root transformation is more likely to be useful when the original distribution is less strongly skewed (Figure \@ref(fig:OWATransformSqrt2)).

```{r OWATransformSqrt2, echo=FALSE, fig.width=5, fig.height=5, fig.cap="Demonstration of the result (upper-left) from applying the square root transformation function (black curve in upper-right) to slightly right-skewed original values (lower-right). The original values here are **not** the same as those in the previous two figures."}
x1 <- rbeta(500,shape1=2,shape2=7);
y1 <- sqrt(x1)
layout(matrix(c(1,3,0,2),nrow=2,byrow=TRUE))
par(mar=c(0,0.1,0.1,0), mgp=c(0,0,0),las=1,tcl=-0.2)
y.hist <- hist(y1,breaks=20,plot=FALSE)
barplot(-y.hist$counts,horiz=TRUE,space=c(0,0),col="gray90",
        xaxt="n",yaxt="n",xlab="",ylab="",xaxs="i",yaxs="i")
mtext("sqrt(Y)",side=3,line=-2)
par(mar=c(0.1,0,0,0.1), mgp=c(0,0,0),las=1,tcl=-0.2)
x.hist <- hist(x1,breaks=20,plot=FALSE)
barplot(-x.hist$counts,space=c(0,0),col="gray90",
         xaxt="n",yaxt="n",xlab="",ylab="",xaxs="i",yaxs="i")
mtext("Y",side=3,line=-10)
par(mar=c(0,0,0,0), mgp=c(0,0,0),las=1,tcl=-0.2)
curve(sqrt(x),min(x1),max(x1),n=501,lwd=2,col="black",
      xlab="",ylab="",bty="n",xaxt="n",yaxt="n",tck=0,xaxs="i",yaxs="i")
xs <- c(0.05,0.1,0.3,0.6)
ys <- sqrt(xs)
segments(xs[1],-10,xs[1],ys[1],col=clrs2[2],lty="dashed")
segments(xs[1],ys[1],0,ys[1],col=clrs2[2],lty="dashed")
points(xs[1],min(y1),col=clrs2[2],pch=16,xpd=NA)
points(min(x1),ys[1],col=clrs2[2],pch=16,xpd=NA)
segments(xs[2],-10,xs[2],ys[2],col=clrs2[2],lty="dashed")
segments(xs[2],ys[2],0,ys[2],col=clrs2[2],lty="dashed")
points(xs[2],min(y1),col=clrs2[2],pch=16,xpd=NA)
points(min(x1),ys[2],col=clrs2[2],pch=16,xpd=NA)
segments(xs[3],-10,xs[3],ys[3],col=clrs2[1],lty="dashed")
segments(xs[3],ys[3],0,ys[3],col=clrs2[1],lty="dashed")
points(xs[3],min(y1),col=clrs2[1],pch=16,xpd=NA)
points(min(x1),ys[3],col=clrs2[1],pch=16,xpd=NA)
segments(xs[4],-10,xs[4],ys[4],col=clrs2[1],lty="dashed")
segments(xs[4],ys[4],0,ys[4],col=clrs2[1],lty="dashed")
points(xs[4],min(y1),col=clrs2[1],pch=16,xpd=NA)
points(min(x1),ys[4],col=clrs2[1],pch=16,xpd=NA)
```

 
&nbsp;

As demonstrated above, the common transformations listed in Table \@ref(tab:CommonTranformations) vary in their ability to "normalize" skewed distributions. Generally the common transformations in Table \@ref(tab:CommonTranformations) are ordered from least to most powerful. In other words, the transformations are ordered from those that "normalize" mildly skewed data to those that "normalize" strongly skewed data.^[Alternatively, the transformations are listed in order from the transformations that "spread out" the small values the least to those that "spread out" the small values the most.]

It is possible to "combine" one of the common powers with the inverse transformation to create a larger array of transformations. For example, $\lambda$=-0.5 is an inverse square-root transformation. These types of transformations are common but less common than those listed in Table 
\@ref(tab:CommonTranformations).

&nbsp;

```{r CommonInvTranformations, echo=FALSE}
tibble(Power=c("&lambda;=-0.5","&lambda;=-0.33","&lambda;=-0.25"),
       Name=c("Inverse Square Root","Inverse Cube Root","Inverse Fourth Root"),
       Formula=c("$Y^{-0.5}=\\frac{1}{\\sqrt{Y}}$",
                 "$Y^{-0.33}=\\frac{1}{\\sqrt[3]{Y}}$",
                 "$Y^{-0.25}=\\frac{1}{\\sqrt[4]{Y}}$"),
       "R Code"=c("<pre style='padding: 2px; display: inline; background: transparent; border: 0;'>df$newvar &lt;- 1/sqrt(df$oldvar)</pre>",
                  "<pre style='padding: 2px; display: inline; background: transparent; border: 0;'>df$newvar &lt;- df$oldvar^(-1/3)</pre>",
                  "<pre style='padding: 2px; display: inline; background: transparent; border: 0;'>df$newvar &lt;- df$oldvar^(-1/4)</pre>")) %>%
  knitr::kable(booktabs=TRUE,escape=FALSE,
               caption="Common inverse power transformations in ANOVAs. These transformations are much less common than those in Table \\@ref(tab:CommonTranformations).") %>%
  kableExtra::kable_classic("hover",full_width=FALSE,html_font=khfont) %>%
  kableExtra::row_spec(0,bold=TRUE)
```

&nbsp;

Power transformations require non-negative and non-zero data. Violations of this restriction can be rectified by adding an amount to all values of the response variable such that all values become positive. This is called *shifting* the data and does not change the shape of the distribution. In addition, power transformations are not effective if the range of values of the response variable is narrow.^[In effect, the power transformation is basically linear over short ranges and, thus, is not effective.]

&nbsp;

### Choosing a Power Transformation {-}
There are several methods to identify the power transformation that is most likely to meet the assumptions of a one-way ANOVA.^[Box and Cox (1964) provided a statistical and graphical method for identifying the appropriate power transformation for the response variable. The details of this method are beyond the scope of this class but, in general, the method searches for a $\lambda$ that minimizes SS<sub>within</sub>). A slightly modified Box and Cox approach is implemented in R by sending a `lm` object to `boxcox()` from the `MASS` package.] One simple method is trial-and-error -- i.e., trying various powers until one is found where the assumptions of the model are most closely met. 

The trial-and-error method is made easier with software. For example, the `assumptionCheck()` function introduced in Section \@ref(testing-assumptions-in-r) can be used to quickly compute the graphs and hypothesis tests for assumption checking *after* the data have been transformed by the power given in the `lambday=` argument. For example, the code and results below show what the assumption checking would look like if the immunoglobulin measurements for the New Zealand opossums had been log transformed.

```{r fig.width=7}
lm1 <- lm(imm~season,data=opp)
assumptionCheck(lm1,lambday=0)  #lambda=0 corresponds to log-transformation
```

&nbsp;

Of course, it was shown in Section \@ref(testing-assumptions-in-r) that the assumptions for a One-Way ANOVA with these data had been met; thus, there is no need to explore a transformation here. However, this shows how easily one can quickly test various transformations for a given set of data.

Note that `assumptionCheck()` only transforms the data "behind-the-scenes." If you want to continue with transformed data then you need to use R code like that shown in the last columns of Tables \@ref(tab:CommonTranformations) and \@ref(tab:CommonInvTranformations).

&nbsp;

## Transformations from Theory
Certain special transformations are common in particular fields of study and are generally well-known to scientists in those fields. An example that crosses many fields is the transformation of proportions or percentages data by using the arcsine square-root function (i.e., $\text{sin}^{-1}(\sqrt{Y})$). Also, a possible power transformation may be chosen from theory related to the response variable. For example, it is common to square-root transform areas and cube-root transform volumes. In addition, discrete counts are often square-root transformed.

&nbsp;

## Interpretations After Transformations
Care must be taken with interpretations following transformations. A few simple rules help in this regard.

First, **tell the reader what transformation you used and how you arrived at it**. In other words, clearly demonstrate that the assumptions were not met on the original scale and demonstrate that they were met on the transformed scale.

Second, when making a conclusion from the One-Way ANOVA p-value, **refer to the transformed response variable in your conclusions**. In other words, say "the mean square root of the response variable differed among groups" rather than "the mean of the response variable differed among groups." It will be implied that the means differed on the original scale, but you strictly tested on the transformed scale so you should be explicit about that here.

Third, **back-transform estimates (and confidence intervals) for "points"**. In One-Way ANOVA this means that you should back-transform estimates of means. For example, if the response variable was log-transformed such that the *mean log of Y* was 1.356 then this should be back-transformed to the original scale with e<sup>1.356</sup> = `r formatC(exp(1.356),format="f",digits=3)`. Similarly if the response variable was square-root transformed such that the *mean square-root of Y* was 1.356 then this should be back-transformed to the original scale with 1.356<sup>2</sup> = `r formatC(1.356^2,format="f",digits=3)`.

<!---
It is known that back-transforming the mean value on the log scale underestimates the mean value on the original scale.^[This stems from the fact that the back-transformed mean value from the log scale is equal to the geometric mean (i.e., the nth root of the product of n values) of the values on the original scale. The geometric mean is always less than the arithmetic mean (i.e., sum of all values divided by n) and, thus, the back-transformed mean always underestimates the arithmetic mean from the original scale.] The most common^[A wide variety of "corrections" for this back-transformation bias have been suggested.] correction for this back-transformation bias^[This correction is derived from the analysis of normal and log-normal distributional theory.] is to multiply the back-transformed value by $e^{\frac{MS_{Within}}{2}}$. This correction factor can be extracted from the `lm` object with `logbtcf()`.

```{r}
logbtcf(lm1)
```
--->

Fourth, **do NOT back-transform "differences" unless those differences are from a log-transformation**. In a One-Way ANOVA this translates into whether or not you can back-transform *differences* in means, as would result from multiple comparisons. For example if the result is a difference in the mean square-root of the response variable between groups then do *NOT* back-transform this value as it can not be back-transformed to anything meaningful. However, if the result is a difference in mean log of the response variable between groups then this *difference* can (and should) be back-transformed to something meaningful.

We know from algebra class that the difference in the logs of two values is the log of the ratio of the two values -- i.e., $log(a)-log(b) = log(\frac{a}{b})$. Thus, back-transforming the difference in log values results in a ratio of values on the original scale -- i.e., $e^{log(a)-log(b)} = e^{log(\frac{a}{b})} = \frac{a}{b}$. Thus, in a One-Way ANOVA, the back-transformed **difference** in group means on the log scale is the **ratio** of group means on the original scale.

For example, suppose that the difference in log means for two groups is 0.5. Back-transforming this value gives $e$<sup>0.5</sup>=`r formatC(exp(0.5),format="f",digits=3)`. Thus, on the original scale, the mean for the first group is `r formatC(exp(0.5),format="f",digits=3)` times larger than the mean for the second group.

Alternatively, suppose that the difference in log means for two groups is -0.5. Back-transforming this value gives $e$<sup>-0.5</sup>=`r formatC(exp(-0.5),format="f",digits=3)`. Thus, on the original scale, the mean for the first group is `r formatC(exp(-0.5),format="f",digits=3)` as large as the mean for the second group. Or, the mean for the *second* group is $\frac{1}{`r formatC(exp(-0.5),format="f",digits=3)`}$=`r formatC(1/exp(-0.5),format="f",digits=3)` times larger than the mean for the *first* group.

Log-transformations are special as they allow both means and differences in means to be back-transformed to the original scale with a meaningful result. Because of this, **the first transformation that you should try in all situations is the log-transformation.** Many times we prefer the log-transformation over other transformations, even if the assumptions are not perfectly met with the log-transformation.

If the log transformation does not work to meet the assumptions of the One-Way ANOVA then you should start with the least strong transformation (i.e., the square root) and successively move through more strong transformations until the assumptions are adequately met.

::: {.tip data-latex=""}
Always try the log-transformation first as it allows for meaningful back-transformations of means and differences.
:::

## Back-Transformations in R
The `emmeans()` function introduced in Section \@ref(multiple-comparisons-in-r) makes back-transformations easy.^[Transformations and back-transformations will be illustrated more in this module's assignment and in Module \@ref(ANOVA1Summary).] While a transformation was not needed for the New Zealand opossums example, let's suppose for illustrative purposes that a square root transformation was used. First, a square root variable is created and a new ANOVA model with this variable is used.

```{r}
opp$sqrtimm <- sqrt(opp$imm)
lm2 <- lm(sqrtimm~season,data=opp)
anova(lm2)
```

Multiple comparisons can be created as before, but there is an advantage to telling `emmeans()` that you used a square root transformation with `tran=` as shown below. Here we can see (see the two notes under the `$emmeans` and `$contrasts` portions of the output) that `summary()` reminds you that the results are on the square root scale. This should help you remember to interpret these results on the square root scale.

```{r warning=FALSE, message=FALSE}
mct <- emmeans(lm2,specs=pairwise~season,tran="sqrt")
( mcsumt <- summary(mct,infer=TRUE) )
```

More importantly if you declare your transformation with `tran=` then you can use `summary()` to **appropriately** back-transform the results by including `type="response"`. Here, the `$emmeans` results are back-transformed as the *"Intervals are back-transformed from the sqrt scale"* note indicates, but you are reminded that the p-values were computed on the transformed scale with the *"Tests are performed on the sqrt scale"* note. The `$contrasts` results are left on the square root scales as noted with *"Note: contrasts are still on the sqrt scale"* because `emmeans()` is smart enough to know that you should not back-transform *differences* when using a square root transformation.

```{r}
( mcsumbt <- summary(mct,infer=TRUE,type="response") )
```

&nbsp;

The back-transformed means in `$emmeans` can be used to construct a plot of means as before, but you must realize that the means are now labeled as "response" (look at the last output above) rather than "emmean" (thus you must change `y=` in `geom_errobar()` and `geom_point()`).

```{r}
ggplot() +
  geom_jitter(data=opp,mapping=aes(x=season,y=imm),alpha=0.25,width=0.05) +
  geom_errorbar(data=mcsumbt$emmeans,
                mapping=aes(x=season,y=response,ymin=lower.CL,ymax=upper.CL),
                size=2,width=0) +
  geom_point(data=mcsumbt$emmeans,mapping=aes(x=season,y=response),
               size=2,pch=21,fill="white") +
  labs(y="Immunoglobulin Concentration",x="Season/Month") +
  theme_NCStats()
```

&nbsp;

Now, look at the same results for a log transformation. Once again the results when not using `type="response"` show a note reminding you that the results are on the log scale. 

```{r}
opp$logimm <- log(opp$imm)
lm3 <- lm(logimm~season,data=opp)
mct <- emmeans(lm2,specs=pairwise~season,tran="log")
( mcsumt <- summary(mct,infer=TRUE) )
```

When `type="response"` is used the `$emmeans` portion of the results are back-transformed with a note again saying so. However, the `$contrasts` portion is now also back-transformed because `emmeans()` is smart enough to know that it is possible (and useful) to back-transform differences from the log scale. In fact the rows in the `$contrasts` portion are appropriately labeled as **ratios** to aid your interpretation.

```{r}
( mcsumbt <- summary(mct,infer=TRUE,type="response") )
```
